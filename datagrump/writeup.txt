Keith Major (keithm2)
Mark Xue (markxue)

CS244 Project 2 Writeup
GitHub repo:
https://github.com/keithm2/CS244_Proj2

Exercise A: The best single window size to maximize the score was a window size of 13, and a average power score of 12.83. The stddev of our measurements was ~.05 for 12,13 and 14


Exercise B:
We implemented AIMD with parameters
window := window + a
window := window x b
for values 
a = 1/ window, 1/(2*window)
b = 1/8, 1/4, 1/2

These schemes produced results much
lower than a fixed window size. Our best result 
was for a = 1/ (2*window), b = 1/4

Exercise C:
We implemented a delay based congestion, initially with AIMD triggered by delay over/under a constant value of 100, 150, and 300ms. These initially scored fairly low, ~ 4.
Out initial modifications were inspired by BBR, in that we wanted to gain signal about network conditions rather than try to fill the window until packets get dropped, as TCP does. 
- First we collected the delays ( sender RTT received - sender packet sent time), and used this to estimate our minRTT, which on the test data was ~46ms. We decided to make the simplifying assumption that propagation delay was relatively static. Updating our minRTT estimate mid-flow would necessarily require that we have a period of sub-optimal throughput to probe that characteristic.
- For each ack received with a delay less than the min * a threshhold value T, we increase the window size by 1. This gives us
very good exponential growth to quickly find the bandwidth limit. Experimentally, we found 1.5 to be the optimal value for T.
- We tried multiple methods of recovery. Initially, we halved the window, but this behavior was too aggressive. Also, it discarded the information about at what size window the delay passes our threshhold. These approaches elevated the score into the 10's
- Out next approach was to preserve the that value as a "window_estimate", while backing the window size off by a multiplicative factor to allow the queue to drain. (we settled on a factor of .75). If we continue to have high RTT's while backing off the window (i.e. window < window_estimate), then we adjust our window estimate downward. We floor the window at a nonzero constant to prevent the backoff from choking off the flow.
- This window estimate was initially based on the time that the ack was received. We then shifted to storing the window size for every sent packet, and retrieving it based on the sequence number we received an ack for, for a more accurate estimate.
- Finally, we implemented a handler that notifies the controller if a timeout occurred, and use that to back off the window estimate. 

Exercise D:
We tried several approaches in parallel, refining both the AIMD and delay-based controllers to scores in the low 20's. Our design was inspired by BBR, in that we wanted to interpret the singals we're receiving to estimate the condition of the flow and respond to avoid congestion. As such, we have three implicit states 
  - exponential growth when we're receiving ack's with low delay
  - recovery (queue drain) when we cross a delay threshold
  - backoff if recovery fails to mitigate delay (i.e. throughput declining)
We also made an attempt to estimate bandwidth at the receiver based on the spacing of the recv_timestamp_acked, but did not come up with a predictive algorithm that substantially improved performance.
Our final submission takes the approach outlined in part C, and further improves upon it:
- We noticed that the delay-based increase would stall if recovering from a network outage. We would periodically resend on timeout, but the first ack's to received after the outage would have artifically long delay times (with the outage included), and we wouldn't begin exponential increase until the queue had cleared and we received ack's from the first packets to be sent after the outage. So the recovery would be delayed by the time to drain the queue.
- Because we actively avoid filling the queue, a timeout is more likely from a network outage or packet drop than a full queue. So on timeout, we save the previous window size, and temporarily decrease the timeout interval to probe for when the network returns. When it does, we restore the previous window, and increase it linearly while masking the delay signal until the pre-outage queue is clear. 




